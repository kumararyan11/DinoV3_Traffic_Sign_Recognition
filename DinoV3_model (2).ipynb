{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrpQm-0KZWoc"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "zip_name = list(uploaded.keys())[0]\n",
        "extract_path = \"/content/dataset\"\n",
        "\n",
        "with zipfile.ZipFile(zip_name, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Dataset extracted to:\", extract_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations opencv-python"
      ],
      "metadata": {
        "id": "75DxVFa_aK67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import albumentations as A\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "TnC4whToabno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augment = A.Compose([\n",
        "    A.RandomResizedCrop(size=(128, 128), scale=(0.6, 1.0)),\n",
        "    A.Rotate(limit=10, p=0.5),\n",
        "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
        "    A.CenterCrop(height=128, width=128, p=0.3),\n",
        "    A.OneOf([\n",
        "        A.GaussianBlur(blur_limit=(3, 5), p=1.0),\n",
        "        A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),\n",
        "    ], p=0.3),\n",
        "    A.CoarseDropout(\n",
        "        max_holes=8,\n",
        "        max_height=16,\n",
        "        max_width=16,\n",
        "        p=0.3\n",
        "    )\n",
        "])"
      ],
      "metadata": {
        "id": "3Wn7zbCOagEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_DIR = \"/content/dataset/custom_fs_ts\"\n",
        "TARGET_SAMPLES = 50"
      ],
      "metadata": {
        "id": "Jmwzwbcnakng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for class_name in os.listdir(DATASET_DIR):\n",
        "    class_path = os.path.join(DATASET_DIR, class_name)\n",
        "    if not os.path.isdir(class_path):\n",
        "        continue\n",
        "\n",
        "    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
        "    current_count = len(images)\n",
        "\n",
        "    if current_count >= TARGET_SAMPLES:\n",
        "        continue\n",
        "\n",
        "    print(f\"Augmenting class '{class_name}': {current_count} → {TARGET_SAMPLES}\")\n",
        "\n",
        "    while len(images) < TARGET_SAMPLES:\n",
        "        img_name = random.choice(images)\n",
        "        img_path = os.path.join(class_path, img_name)\n",
        "\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        augmented = augment(image=image)[\"image\"]\n",
        "\n",
        "        new_name = f\"aug_{len(images)}_{img_name}\"\n",
        "        save_path = os.path.join(class_path, new_name)\n",
        "\n",
        "        cv2.imwrite(save_path, cv2.cvtColor(augmented, cv2.COLOR_RGB2BGR))\n",
        "        images.append(new_name)"
      ],
      "metadata": {
        "id": "o_ZwHe9Far5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for class_name in sorted(os.listdir(DATASET_DIR)):\n",
        "    class_path = os.path.join(DATASET_DIR, class_name)\n",
        "    if os.path.isdir(class_path):\n",
        "        print(class_name, \"→\", len(os.listdir(class_path)))"
      ],
      "metadata": {
        "id": "HVyiEa_Ra59Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")"
      ],
      "metadata": {
        "id": "mz0iMjdabOLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/facebookresearch/dinov3.git\n",
        "%cd dinov3"
      ],
      "metadata": {
        "id": "d4X02XHRbW9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -r requirements.txt\n",
        "!pip install -q einops timm pillow tqdm"
      ],
      "metadata": {
        "id": "B90WZnQLbdQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "JGXTcAyibhsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers accelerate safetensors"
      ],
      "metadata": {
        "id": "CmG9kEE1bnye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoImageProcessor\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(\n",
        "    \"facebook/dinov3-vits16-pretrain-lvd1689m\"\n",
        ")\n",
        "\n",
        "model = AutoModel.from_pretrained(\n",
        "    \"facebook/dinov3-vits16-pretrain-lvd1689m\"\n",
        ").to(DEVICE)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(\"DINOv3 loaded successfully via Hugging Face\")"
      ],
      "metadata": {
        "id": "odSYW8C-b6bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(\"/content/dataset/custom_fs_ts\", transform=transform)\n",
        "\n",
        "dataloader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "num_classes = len(dataset.classes)\n",
        "print(\"Classes:\", dataset.classes)"
      ],
      "metadata": {
        "id": "euxmPW3ecAiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "\n",
        "encoder = AutoModel.from_pretrained(\n",
        "    \"facebook/dinov3-vits16-pretrain-lvd1689m\"\n",
        ").to(DEVICE)\n",
        "\n",
        "encoder.eval()\n",
        "\n",
        "for param in encoder.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "print(\"DINOv3 encoder loaded and frozen\")"
      ],
      "metadata": {
        "id": "RCXYm63tcMuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def split_into_quadrants(images):\n",
        "\n",
        "    B, C, H, W = images.shape\n",
        "    h_mid, w_mid = H // 2, W // 2\n",
        "\n",
        "    q1 = images[:, :, :h_mid, :w_mid]\n",
        "    q2 = images[:, :, :h_mid, w_mid:]\n",
        "    q3 = images[:, :, h_mid:, :w_mid]\n",
        "    q4 = images[:, :, h_mid:, w_mid:]\n",
        "\n",
        "    return [q1, q2, q3, q4]"
      ],
      "metadata": {
        "id": "OcrIAgXCcPx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.eval()\n",
        "\n",
        "def extract_quadrant_features(images):\n",
        "    with torch.no_grad():\n",
        "        full_out = encoder(pixel_values=images)\n",
        "        cls_full = full_out.last_hidden_state[:, 0]\n",
        "        quadrants = split_into_quadrants(images)\n",
        "        quad_cls_tokens = []\n",
        "        for q in quadrants:\n",
        "            out = encoder(pixel_values=q)\n",
        "            cls_q = out.last_hidden_state[:, 0]\n",
        "            quad_cls_tokens.append(cls_q)\n",
        "        cls_quadrant_mean = torch.stack(quad_cls_tokens, dim=1).mean(dim=1)\n",
        "        final_features = torch.cat([cls_full, cls_quadrant_mean], dim=1)\n",
        "\n",
        "    return final_features"
      ],
      "metadata": {
        "id": "gCr94kIRcTUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "for imgs, lbls in tqdm(dataloader):\n",
        "    imgs = imgs.to(DEVICE)\n",
        "    feats = extract_quadrant_features(imgs)\n",
        "\n",
        "    features.append(feats.cpu())\n",
        "    labels.append(lbls)\n",
        "\n",
        "X = torch.cat(features)\n",
        "y = torch.cat(labels)\n",
        "\n",
        "print(\"Feature shape:\", X.shape)"
      ],
      "metadata": {
        "id": "pUToadMbcXZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_train = X_train.to(DEVICE)\n",
        "X_val = X_val.to(DEVICE)\n",
        "y_train = y_train.to(DEVICE)\n",
        "y_val = y_val.to(DEVICE)"
      ],
      "metadata": {
        "id": "bZIV-qXZcaYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "emb_dim = X.shape[1] // 2\n",
        "num_classes = len(dataset.classes)\n",
        "\n",
        "classifier = nn.Linear(2 * emb_dim, num_classes).to(DEVICE)"
      ],
      "metadata": {
        "id": "jgWwKfNccfhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "full_dataset = TensorDataset(X, y)"
      ],
      "metadata": {
        "id": "yEaUCKgMcjFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "val_ratio = 0.2\n",
        "val_size = int(len(full_dataset) * val_ratio)\n",
        "train_size = len(full_dataset) - val_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(\n",
        "    full_dataset, [train_size, val_size]\n",
        ")"
      ],
      "metadata": {
        "id": "H_lRArI2cmHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "17LhEo0Kcp1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "gm7pNop7ctQs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "\n",
        "    classifier.train()\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch = X_batch.to(DEVICE)\n",
        "        y_batch = y_batch.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = classifier(X_batch)\n",
        "        loss = criterion(logits, y_batch)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * y_batch.size(0)\n",
        "        correct += (logits.argmax(dim=1) == y_batch).sum().item()\n",
        "        total += y_batch.size(0)\n",
        "\n",
        "    train_loss /= total\n",
        "    train_acc = correct / total\n",
        "\n",
        "\n",
        "    classifier.eval()\n",
        "    val_correct = 0\n",
        "    val_total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch = X_batch.to(DEVICE)\n",
        "            y_batch = y_batch.to(DEVICE)\n",
        "\n",
        "            logits = classifier(X_batch)\n",
        "            val_correct += (logits.argmax(dim=1) == y_batch).sum().item()\n",
        "            val_total += y_batch.size(0)\n",
        "\n",
        "    val_acc = val_correct / val_total\n",
        "\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(\n",
        "            f\"Epoch [{epoch+1}/{epochs}] \"\n",
        "            f\"Train Loss: {train_loss:.4f} \"\n",
        "            f\"Train Acc: {train_acc:.4f} \"\n",
        "            f\"Val Acc: {val_acc:.4f}\"\n",
        "        )"
      ],
      "metadata": {
        "id": "Juk_Km6TcxLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "    \"encoder\": encoder.state_dict(),\n",
        "    \"classifier\": classifier.state_dict(),\n",
        "    \"classes\": dataset.classes\n",
        "}, \"dinov3_linear_probe.pth\")\n",
        "\n",
        "print(\"Model saved successfully\")"
      ],
      "metadata": {
        "id": "LPuWJhg9c3Rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "img_path = \"/content/dataset/custom_fs_ts/EA/Dropped Image (2).png\"\n",
        "\n",
        "img = Image.open(img_path).convert(\"RGB\")\n",
        "img = transform(img).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "encoder.eval()\n",
        "classifier.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "\n",
        "    full_out = encoder(pixel_values=img)\n",
        "    cls_full = full_out.last_hidden_state[:, 0]\n",
        "\n",
        "\n",
        "    quadrants = split_into_quadrants(img)\n",
        "    quad_cls_tokens = []\n",
        "\n",
        "    for q in quadrants:\n",
        "        out = encoder(pixel_values=q)\n",
        "        quad_cls_tokens.append(out.last_hidden_state[:, 0])\n",
        "\n",
        "    cls_quad_mean = torch.stack(quad_cls_tokens, dim=1).mean(dim=1)\n",
        "\n",
        "\n",
        "    feat = torch.cat([cls_full, cls_quad_mean], dim=1)\n",
        "\n",
        "    pred = classifier(feat).argmax(dim=1)\n",
        "\n",
        "print(\"Predicted class:\", dataset.classes[pred.item()])"
      ],
      "metadata": {
        "id": "iLls6v5adFgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2luQ8usVY_0x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
